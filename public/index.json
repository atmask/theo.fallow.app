[{"content":"Big Idea Autoscaling is a key technology that builds on the elasticity of distributed cloud computing. To understand autoscaling in the Kubernetes environment, it will be important to understand the basic enabling technologies: containerization and distributed computing. By leveraging hese two technologies, Kubernetes supports horizontal pod autoscaling (hpa) and cluster autoscaling. In this post I will dive into some brief background on Containerization and distributed computing (just enough to be dangerous) and then take a more focused look at the horizontal autoscaling functionality of Kubernetes.\nContainers and Distirbuted Computing As I mentioned before, I want to touch briefly on the enabling technologies of autoscaling in the Kubernetes envrionment specifically. These two technologies are containers and distirbuted computation. Let\u0026rsquo;s look at each in turn\u0026hellip;\nContainers Containers are often compared to VMs. They are both technologies that virtualize the resources of an underlying machine. Virtualization means that one homogenous resource is \u0026ldquo;virtually\u0026rdquo; represented as multiple resources. Unlike a VM which which virtualizes an entire machine including the hardware layers, a container on the other hand, virtualizes the execution environment (os, packages, libraries). Whereas the VM is equivalent to creating a habitable environment on Mars, a container is equivalent to putting on a space suit.\nWhat the container represents is a unit of deployment. It wraps an application so that it can be deployed and run predicatably on any machine without being concerned about the details of the underlying machine on which the container runs. One of these underlying details that a container is not concerned about is other containers running on the machine. This means that the container is an isolated execution environment consuming virtualized resources from an underlying physical machine\nDistributed Computing Distributed computing is a natural product of the boom of networking technology that came out of the Bay area in the 80\u0026rsquo;s and 90\u0026rsquo;s. It was the end of the era of mainframe computing, in which time-sharing was a key mechnism for partitioning the computation of a single machine among many users by sharing processing time. With improving network capabilities and an increasingly available fleet of machine often with idle compute, companies tried to develop tools that presented a collection of machines as a single service. For many years this was stumbling breakthroughs with new technologies like remote procedure calls (RPC), network file systems (NFS), Andrew File System (AFS), Locus, Sprite, Plan 9, and on.\nThe goal that many of these early systems tried to achieve was distributing an application acorss many machines. They attempted to present a single OS interface for many machines. In this paradigm, the process was the unit of distirbution.\nOver more time than I care to write about here, the paradigm evolved. There was a shift away from trying to distribute processes across machine and towards distributing computation that was embarassingly parallel across machines.\n📖 Embarrassingly parallel tasks are tasks that it takes relativlely little to no work to split the problem in parallel execution environments to run computation. If needed, results produced by tasks executed in parallel can be aggregated afterwards.\nA perfect example for a computation unit to run in parallel across many machines is the modern container. Applications run across a fleet of machines and stateless HTTP requests can get handled by an application in any container or applications can consume from a central queue and process large numbers of jobs in parallel.\nKubernetes With a better understanding of containers and distributed computing it may be easier to step back and look at Kubernetes. Kubernetes, labelled as a container orechstraion system, is tool that presents a single interface (control plane) for users to run container-based applications across a large collection of nodes.\nAutoscaling Cloud providers have a signifcant amount of computational resources at their disposal. As result, these providers, give user\u0026rsquo;s a benefit known as elatisity. Elastisity is the ability to provision or de-allocated resources on demand and based on need. This is a unique feature of cloud environments that contrasts the often slow acquisition time and upfront captial that needs to beinvested to procure on-prem equipment.\nAutoscaling is data-driven approach to provisioning and de-provisioning resources based on active load on a system at a given point in time. The two forms of autoscaling that we will look at further on are horizontal pod autocaling and cluster autoscaling. These are both forms of autoscaling in Kubernetes that would be descrbed as forms of horizontal scaling. Horizontal scaling, sometimes known as scaling out, refers to the addition of more discrete computation units (i.e. add more nodes or pods). In contrast, vertical scaling, also known as scaling up, means increasing the available hardware capabilites of the existing machines (add more cpu, memory).\nHorizontal Pod Autoscaling (HPA) By default, any Kubernetes workload you deploy that creates pods such as a deployment or statefulset is not going to autoscale. Usually in a basic configuration the number of desired pods will be statically set by defining the replicas field on the controller. However, the Kubernetes API supports a Horizontal Pod Autoscaler (HPA) resource. Rather than generating a fixed number of pods, the HPA will increase or decrease the number of pods for the workload (remember, horizontal scaling) based on the observed resource utilization of the pods in that workload.\nMore specifically, the horizontal pod autoscaler is a controller shipped as part of the Kubernetes controller manager, a daemon that embeds the default controllers shipped with Kubernetes. The HPA resources that we create via the Kubernetes API will specify configuration for this controller to manager our workloads.\nWhen we talk about a controller in Kubernetes, we are talking about a control loop process that continuosuly evaluates the current state of the system, compares it to some desired state, and makes changes as required. In the case of the HPA, an metric is observed, compared to a threshold, and pods are scaled up or down (or not at all).\nScaling Based on Resource Usage In order to scale a pod based on observed resource usage you need to specify resource requests on a pod. Resources requests are a field that can be optionally defined as part of your pod template. The resource requests specify the amount of cpu and/or memory that the pod needs to run. Defining resource requests is considered best practice because they enable the Cluster Scheduler to determine whether there are any nodes with sufficient resources when placing a pod.\nDefining the resource requests for a pod is necessary for enabling HPA because the only resource metrics that the HPA can scale based on (without the more involved process of setting up custom pod or object metrics) are the observed usage of the requested cpu and memory resources. Here is an example HPA that scales based on the % utilization of CPU:\napiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: nginx spec: ## Defined the workload we want to scale scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nginx minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 60 what you see above is an HPA that scales in a new pof for Deployment workload named nginx whenever the average cpu utilization is above 60%. An alternative to the target.type: Utilization is the target.type: AverageValue. Using the latter you can specify an exact value for resource usage, such as the number of milicores, or Gb of memory.\n💡 Important: It is a common misperception when seeing a pod\u0026rsquo;s % cpu usage or % memory usage to assume that the ratio is with respect to the resources available on the underlying node. In reality, more often that not this value is with respect to the resources requests specified on the pod spec. Further, the average utilization measured and compared to the threshold defined in the HPA is the resource usages of all the pods added and averaged. This means a situation can result where one pod\u0026rsquo;s utilization is above the threshold but no new pods is scaled in since the average across all pods in the workload are below the threshold.\nThe Autoscaling Algorithm In the most basic scenario, scaling is triggered based on the ratio between the configured threshold metric value and the observed metric value. At any point in time you can calculate the number of replicas the autoscaler should have by using the same calulation:\ndesiredReplicas = ceil[currentReplicas * ( currentMetricValue / thresholdMetricValue )] Based on the above calculation we know that a scale up occurs as soon as the thresholdMetricValue is surpassed. However, because we use ceil, a scale down does necessarily happen as soon as we drop below the threshold. This provides some stability to the scaling. Here is an example:\n## Initial State currentReplicas = 2 currentCPU = 100 #millicores thresholdCPU = 100 ## No scale up desiredReplicas = ceil(2 * (100 / 100)) = 2 ## Scale Up currentCPU = 101 desiredReplicas = ceil(2 * (101 / 100)) = ceil(2 * 1.01) = ceil(2.02) = 3 ## Now 3 replicas ## No Scale Down currentReplicas = 3 currentCPU = 90 desiredReplicas = ceil(3 * (90 / 100)) = ceil(3 * 90) = ceil(2.7) = 3 Scaling with Multiple Metrics Scaling is not limited to one metric. You can specify multiple metric thresholds for scaling. When multiple metrics are defined, the HPA will evaluate each metric, select the metric that results in the gratest number of running pods, and scale if needed. If any metric cannot be calculated then scaling is skipped for that round of calulations.\nScaling based on Custom Metrics It\u0026rsquo;s not unlikely that you may want to scale your workload based on metrics other than CPU or memory. To understand how to do this, let\u0026rsquo;s deepen our understanding of how the horizontal pod autoscaler gathers metrics.\nContrary to the original diagram provided above, there is actually third component involved in the control loop of the horizontal pod autoscaler. That component is the metrics registry.\nThe metrics registry is an extension to the Kubernetes API that serves metrics through a standard interface to any clients in the cluster. Using Prometheus Metrics and the Prometheus Metrics Adapter, it is possible to expose custom metrics via the Metrics Server API for scaling purposes.\nCluster Autoscaling Scaling pods horizontally is great but its benefits are limited by the availability of the resources needed to actually run those pods. If your cluster consists of 5 nodes, then in all likely-hood, you can\u0026rsquo;t support scaling up to tens or hundreds of a resource intensive pods for a resource intensive workload. There is where cluster autoscaling — the obverse of horizontal pod autoscaling — comes in to support a fully elastic workload. Cluster autoscaling is the ability have your cluster dynamically add or remove nodes from your cluster based on demand. In a cloud-based environment, vendors generally have a cluster autoscaling solution that integrates with their Virtual Machine solutions to automatically provision or de-provision nodes.\nProvisioning New Nodes As I said above, cluster autoscaling is the ability have your cluster dynamically add or remove nodes from your cluster based on demand. However, we need to take a look at how demand is evaluated in the case of the cluster autoscaler. Unlike horizontal pod autoscaling, which scales the pods for a workload based on observed resource usage, the cluster autoscaler does not operate based on the observed resource usage of nodes. Instead the cluster autoscaler operates based on the ability to schedule a new pod with the current available resources.\nWhen the cluster shceduler attemps to schedule a pod, it will look at the resource requests of that pod, filter out nodes from consideration based on any node taint/tolerations, and then check if any node has sufficient resources to host the pod. If there are no nodes with sufficient resources available to satisfy those requests, then the autoscaler will be triggered to add an aditional node.\nFinding a Schedulable Node Finding a scheduleable node is a two part process that consists of:\nFiltering: The Scheduler filters out nodes that the pod cannot be scheduled to based on node selectors, taints, and resources Scoring: The feasible nodes are passed through a series of scoring functions for determing which node should be used for placement For this post we\u0026rsquo;ll focus on the resource filtering aspect of pod scheduling.\nAfter the scheduler filters out nodes based on taints and node selectors, the scheduler will compute the available capacity of each node. To calculate the available capacity of a node, you subtract the the resources allocated for the system (kubelet, OS processes) and the resources requests by currently running pods on that node. It is important to note that no real-time observed usage metrics are used for this process. If the node is determined to have available capacity for the requests of the pod then it is considered viable for scheduling and will be scored in the next phase.\nThe above description makes sense in a perfect world where requests are set on all of our workloads but that is not always the case. Given this fact, there are some nuances around pods without resource requests to understand.\nPods that do not have requests specified are considered to have a BestEffort Quality of Service (QoS). The resource usage of pods with a BestEffort QoS running on a node are not considered as part of the calculation used by the scheduler to determine the available capacity of a node. However, if a node is under pressure for resources, BestEffort QoS pods are the first to be evicted from the node. If the scheduler is trying to schedule a pod with a BestEffort QoS, the pod will be placed on any node that has available capacity. While the scheduler does not look at metrics, the kubelet on each node does. If a node is under pressure for resource, the kubelet will begin to mark pods for eviction beginning with BestEffot QoS pods.\nLet\u0026rsquo;s take a look at an visual example of this:\nWe can observe a few key things here:\nNode 1 is elimated due to having a taint that the new pod does not tolerate The BestEffort QoS pod on node 2 does not decrease the computed available capacity of that node which can lead to over scheduling Node 2 and Node 3 are feasible for scheduling the pod But what happens if the pod couldn\u0026rsquo;t be scheduled? In that case, the cluster autoscaler will be responsible for adding a new node into the cluster. If there is only one node type/size that can be scaled then scaling is trivial and a new node is provisioned. If there are multiple node pools available to this cluster with different types or sizes of node that could be added then the cluster autoscaler must select one of the available node pools to scale. How this selection is performed is the next topic that will be covered.\nExpanders If a pod cannot be scheduled and a new node must be provisioned for a cluster with multiple node pools then a decision needs to made about what type of node to add to the cluster. Often times you will have multiple node pools available in your cluster. These node pools could contain different sizes of node or nodes with differnet capabilities and hardware specifications. In this case, the question of how the Cluster Autoscaler selects which node pool to scale arises. This is the problem that expanders solve. Expanders are strategies configured on the autoscaler profile that determine which node type to select when scaling up.\nNote: If you only have one pool of node types available for scaling up then scaling in a new node is trivial and expanders do not apply.\nCluster Autoscaler implementations in the various cloud providers support a configuration for expanders. Expanders are strategies for solving this problem. Often there will be expanders for scaling based on the following:\nprice: scale the node pool with the cheapest node type available. priority: scale node pools based on user assigned priority random least-waste: Scales the node pool that would have the least idle CPU after all the pending pods are scheduled For a full overview of expanders read up here\nDe-provisioning Nodes CA Profile ","permalink":"http://localhost:1313/posts/05-intro-to-k8s-autoscaling/","summary":"Big Idea Autoscaling is a key technology that builds on the elasticity of distributed cloud computing. To understand autoscaling in the Kubernetes environment, it will be important to understand the basic enabling technologies: containerization and distributed computing. By leveraging hese two technologies, Kubernetes supports horizontal pod autoscaling (hpa) and cluster autoscaling. In this post I will dive into some brief background on Containerization and distributed computing (just enough to be dangerous) and then take a more focused look at the horizontal autoscaling functionality of Kubernetes.","title":"Intro to K8s Autoscaling"},{"content":"Big Idea The goal of this post will be to give an overview of IPv4 addresses. My aim is to do this incrementally by first covering the anatomy of an IPv4 address in its base 10 and binary representations. Second, I will look at CIDR block subnetting and subnet masks. Thirdly, I will append some helpful formulas for working with IP addresses.\nBackground \u0026amp; History Before diving into the anatomy of IPv4 let\u0026rsquo;s turn back the page. In 1981, RFC 791 was published. This document outlined the Internet Protocol and the IPv4 addressing scheme that would be used to uniquely identify and locate machines on the Internet.\nThe ability to uniquely identify devices on a network is incredibly important. This ability is how we route packages from one machine to another across a network. On a network, a given IP must be uniquely assigned to a single device for seamless communication. The concept of the IP address that we will explore today is one part of a larger layered model (known as the OSI model) that facilitates the transfer of data over a network.\nEngineers at the time that RCF 791 was published did not realize the magnitude of devices that would one day be connected to the Internet. This has become a problem in recent times as there are a fixed number of globally unique IPv4 addresses. As the number of devices has grown to surpass the number of available IPs, engineers have had to apply solutions such as Network Address Translation (NAT) to use IPs more effectively. This exhaustion of IPv4 addresses has also led to a second standard in IP addressing known as IPv6 which is becoming increasingly common. IPv6 functions in a similar fashion to IPv4 but has roughly 3.4×10^38 addresses compared to IPv4\u0026rsquo;s approximately 4x10^9 addresses.\nAnatomy of IPv4 An IPv4 address that you see may look like the following: 192.168.0.2. This address represents 32 bits in a base 10 format. Those 32 bits are divided into 4 groups known as octets. Each octet contains 8 bits. The four octets correspond to the four \u0026lsquo;.\u0026rsquo; separated parts of the IP address in our example. When working with IPv4, it is often easier to convert the base 10 represenation of the IP address into binary. This means that we could represent the above IP as follows:\n192.168.0.2\n1100 0000 - 1010 1000 - 0000 0000 - 0000 0010\nThe lowest value in an octet is 0. In binary, this would correspond to an octet of all 0 bits: 0000 0000. The highest value in an octet is 255. This is the highest number that can be represented with 8 bits (2^8-1): 1111 1111. We subtract 1 from 2^8 since 2^8 represents the total unique values that can be represented with 8 bits, however, 0 is one of those values that can be represented meaning the values range from 0-255 (i.e. 2^8-1).\nSubnetting IPv4 It is common, when working with IP addresses in the cloud, that you want to create groups of sequential IPs that you can dedicate to a certain use. Often when people talk about these groups you will hear things like \u0026ldquo;IP space\u0026rdquo;, \u0026ldquo;CIDR block\u0026rdquo;, \u0026ldquo;subnet\u0026rdquo;, or \u0026ldquo;address range\u0026rdquo;. The key idea is that you want to carve out a broad range of IPs and divide those IPs in an effective way. This always involves balancing two variables: the number of subnets you want to create and the number of devices in each subnet. This segues into the next idea which is that each 32-bit IPv4 address is divided into two parts. A network portion and a host portion.\nLet\u0026rsquo;s say we have a network consisting of all of the IP addresses between 10.0.0.0 to 10.255.255.255. In that range of IPs, we see that the leading octet always has a value of 10 on the network (ex. 10.0.0.1, 10.100.29.0). This is because the first octet, that is the first 8 bits of this IP address, are fixed and represent the network portion of the IP address. The following 3 octets can change in value and represent the host portion of the IP. Let\u0026rsquo;s look at this in binary:\n10.0.0.0\n0000 1010 - 0000 0000 - 0000 0000 - 0000 0000\n(The leading 8 bits in bold are fixed and make up the network protion of the address)\nIn this example, the host portion allows 16,777,216 hosts. It is very likely that we don\u0026rsquo;t want to dedicate all of those addresses to a single project. Instead, we want to partition out a section of IPs for our use-case.\nCIDR Notation When it comes to dividing an IP into the network portion and the host portion, the network portion is always represented by some leading number of bits. How many leading bits are fixed is often up to you, the developer, to decide. This means that we need a standard way to tell others how many leading bits are fixed and represent the network portion of the address. To solve this problem we will introduce CIDR notation. CIDR notation stands for Classless Inter-domain Routing and is a standard way to tell others how many leading bits make up the network portion of an address. The name, classless inter-domain routing is a nod to an earlier and antiquated method of partitioning IP addresses known as class-based routing.\nUsing CIDR notation you will append an /x to the end of your network IP. The value of x dictates how many leading bits dictate the network portion of the address. In the example above we would represent our address range of 10.0.0.0 to 10.255.255.255 as 10.0.0.0/8.\nThe number of leading bits does not need to cleanly divide the octets. This means you can have CIDR blocks other than /8, /16, /24, or /32. You could also have a CIDR block like /12. When creating a CIDR block that splits and octet the binary notation becomes increasingly helpful. Take a look at the following:\n10.0.0.0/12\n0000 1010 - 0000 0000 - 0000 0000 - 0000 0000\n10.0.x.x -\u0026gt; 10.15.x.x\nSubnet Masks Subnet masks are another common notation for indicating the network and host portions of an IPv4 address. Subnet masks resemble an IP address. They are 32 bits divided into 4 otects. Subnet masks work like a kind of filter against which the 32 bits of an IP address can be OR\u0026rsquo;ed. To make this concrete let\u0026rsquo;s take the IP address 192.168.0.2 and the subnet mask 255.255.255.0 and find the host and network portions.\nType Decimal Binary IPv4 192.168.0.2 1100 0000 - 1010 1000 - 0000 0000 - 0000 0010 Subnet Mask 255.255.255.0 1111 1111 - 1111 1111 - 1111 1111 - 0000 0000 OR 192.168.0.0 1100 0000 - 1010 1000 - 0000 0000 - 0000 0000 By OR-ing the bits of the IP with the bits of the subnet mask, we are left with the host network. There is a condition for subnet masks. A subnet mask is 32 bits, which is always a contiguous series of 1s followed by a contiguous series of 0s. This means that, when converted to binary, a subnet mask will have a first half that is all 1\u0026rsquo;s and a second half that is all 0\u0026rsquo;s.\nHost Address and Broadcast Address An important thing to know about subnets is that two IPs in any given subnet are always reserved. The first is the lowest IP in that subnet. The lowest IP in a subnet is the network address. The network address uniquely identifies the network and is not used for hosts in that network. The second reserved address is the broadcast address. The broadcast address is the highest/last IP address available in the network. Any message sent to the broadcast address of a network is sent to all machines on that network.\nUseful Formulas and Notes Number of IPs in a CIDR Block 2^(32 - CIDR block leading bits)\nNumber of Unique Address of x Bits 2^x\nLargest IP of x Bits 2^(x - 1) # 0 is one of the available values so subtract 1\nResizing CIDR blocks Every time you decrease the number of bits in the network portion of an address you add bits to the host portion. This means that more IPs are available. For every bit that you decrease the network potion by, the number of available IPs in the host portion of the network number is doubled.\n","permalink":"http://localhost:1313/posts/04-understanding-ipv4-addressing/","summary":"Big Idea The goal of this post will be to give an overview of IPv4 addresses. My aim is to do this incrementally by first covering the anatomy of an IPv4 address in its base 10 and binary representations. Second, I will look at CIDR block subnetting and subnet masks. Thirdly, I will append some helpful formulas for working with IP addresses.\nBackground \u0026amp; History Before diving into the anatomy of IPv4 let\u0026rsquo;s turn back the page.","title":"Understanding CIDR Blocks and IPv4 Addressing"},{"content":"Big Idea I was recently tasked with configuring automated load tests to validate the health of a service under load and to identify bottlenecks and limits at which the service became overloaded. Up until this point, I had not worked first-hand with any load-testing frameworks. Although there are many great load testing tools out there like JMeter, K6s, and Locust, I decided to get started with Locust as it is a framework I had heard of before and is a pure Python framework (Python is the language I think in right now). To rewind, load testing frameworks allow engineers to programmatically produce stress on a system by simulating a large volume of incoming requests to that system. Requests can be, but are not limited to, HTTP calls. You can also load test with gRPC or IoT protocols like MQTT. Locust is an open-source Python-based load testing framework. With Locust, all user behaviour is defined in Python code and tests can be executed from a single machine or distributed across many machines.\nIf you\u0026rsquo;d like to borrow a template for starting a Locust project, you can find my Locust boilerplate project on GitHub: https://github.com/atmask/locust-template\nSetting Up my First Locust Project When setting up my Locust project, I decided to manage my project with poetry. poetry is a tool for managing the different aspect of Python projects, such as dependencies and packaging, in a deterministic way.\nTo get started, I set up the following basic project structure:\nlocust-tests │ ├── LoadTests │ ├── __init__.py │ ├── locustfile.py │ └── settings.py └── README.md Note: # Do not worry about the contents of the LoadTests/ dir. I have added them but for now, the files can be left blank\nInstalling Dependencies and Virtual Environment Setup I won\u0026rsquo;t go into detail about the installation and general usage of poetry here but will provide the steps to get the project up and running.\n🛠️ Note: If you don\u0026rsquo;t care to use poetry, you can also just add a venv using python -m venv .venv and set up the following requirements.txt file for something quick and easy:\n# requirements.txt file locust First, initialize the new poetry managed project from /locust-tests:\npoetry init I like to have my virtual environment contained within my project so I also add a poetry.toml file to the root of my project with:\n[virtualenvs] in-project = true Finally, you can add the dependencies using potery add. This will install the package along with its dependencies and store the dependencies in the pyproject.toml file and a poetry.lock file.\npoetry add locust Your project structure should now resemble the following:\nlocust-tests │ ├── LoadTests │ ├── __init__.py │ ├── locustfile.py │ └── settings.py ├── README.md ├── poetry.lock ├── poetry.toml ├── pyproject.toml └── struct.txt Writing a Simple Locust Test To get started let\u0026rsquo;s write a simple test that visits the main page for a website. To do this we have to write a class that represents some type of \u0026ldquo;user\u0026rdquo; of the system. We will give this user behaviours by writing functions and decorating those functions with @task decorator. When Locust runs, it will, by default, find all of the defined User classes in locutfile.py and randomly execute the @task decorated functions.\nHere is an example for a first test. This example User class extends Locust\u0026rsquo;s base HttpUser class and defines a behaviour/task for visiting the root page of a site.\n# locustfile.py from locust import HttpUser, task class WebUser(HttpUser): @task def visit_main_page(self): \u0026#39;\u0026#39;\u0026#39;Visit the main page of the domain\u0026#39;\u0026#39;\u0026#39; self.client.get(\u0026#34;/\u0026#34;) You may notice that the GET request does not specify a domain or scheme. This is because the Locust UI allows you to configure the domain you will run requests against interactively. Locust then builds an Environment object that gets passed to the HttpUser class (and any class that extends it) in the constructor. The client then automatically uses the configured domain as the base url for the requests.\nYou can now launch the Locust server and run this first Locust test!\n## If you are using poetry poetry shell ## If you are using a venv . .venv/bin/activate ## Enter the project containing the locustfile.py and run the locust server cd LoadTests/ locust ## Expected output \u0026gt;\u0026gt;\u0026gt; Starting web interface at http://0.0.0.0:8089 You should be able to navigate to the above url in your browser and see the Locust UI. Managing Different Types of Application Use Cases In reality, you\u0026rsquo;ll want to add more complex behaviours to your load tests. You likely also want to group the behaviours/tasks that you are automating to run against your server into different groups of user classes. This will let you define the behaviours of users in different scenarios such as an unauthenticated user vs an authenticated user.\nI will provide a project structure that seems scalable and maintainable to me from a code perspective for expanding to this next part of developing a Locust project. This project structure adds an additional package named behaviours. You could name it users but I chose what I felt was most clear to me. In this package, we can define different subclasses of HttpUser and group related behaviours inside User classes. In this example, I have added an unauthenticated user and an authenticated user.\nlocust-tests │ ├── LoadTests │ ├── behaviours │ │ ├── __init__.py │ │ ├── unauthenticatedUser.py │ │ └── authenticatedUser.py │ ├── __init__.py │ ├── locustfile.py │ └── settings.py ├── README.md ├── poetry.lock ├── poetry.toml ├── pyproject.toml └── struct.txt Here is some example content for the unauthenticated user:\n## LoadTests/behaviours/unauthenticatedUser.py from locust import HttpUser, task class UnauthenticatedUser(HttpUser): @task def visit_main_page(self): \u0026#39;\u0026#39;\u0026#39;Visit the main page of the domain\u0026#39;\u0026#39;\u0026#39; self.client.get(\u0026#34;/\u0026#34;) @task def visit_register_page(self): \u0026#39;\u0026#39;\u0026#39;Visit the registration page\u0026#39;\u0026#39;\u0026#39; self.client.get(\u0026#34;/register\u0026#34;) Now, you might be wondering: \u0026ldquo;What about an authenticated user? How do I manage logging in? Do I need to do this before each test?\u0026rdquo;. All great questions. I will first answer them with a naive solution and then improve upon that answer below.\nManaging Authentication and Secrets Let\u0026rsquo;s take a look at setting up our authenticatedUser:\n## LoadTests/behaviours/authenticatedUser.py import HttpUser, task from dataclasses import dataclass from requests.auth import HTTPBasicAuth @dataclass class UserAuthConfig: username: str password: str class AuthenticatedUser(HttpUser): def __init__(self, user_auth_config: UserAuthConfig, *args, **kwargs): self._user_auth_config = user_auth_config super().__init__(*args, **kwargs) def on_start(self): \u0026#39;\u0026#39;\u0026#39;Locust call on_start when a simulated user starts running\u0026#39;\u0026#39;\u0026#39; self.login() def login(self): \u0026#39;\u0026#39;\u0026#39;Login using basic auth\u0026#39;\u0026#39;\u0026#39; response = requests.get( \u0026#39;/login\u0026#39;, auth=HTTPBasicAuth(self._user_auth_config.username, self._user_auth_config.password) ) @task def visit_profile(self): \u0026#39;\u0026#39;\u0026#39;Visit the authenticated user\u0026#39;s profile\u0026#39;\u0026#39;\u0026#39; self.client.get(\u0026#39;/profile\u0026#39;) This is great! But what if your application supports different authentication mechanisms? And what if multiple different user\u0026rsquo;s all need authentication? This is where we can improve the above example by adding some additional structuring to the project to keep things DRY and consistent. This will be done by creating an interface that must be implemented by all authenticated user classes and adding a couple of authentication base classes to implement the interface with different auth mechanisms.\nLet\u0026rsquo;s look at the new project structure with the changes that will be introduced:\nlocust-tests │ ├── LoadTests │ ├── authentication │ │ ├── __init__.py │ │ ├── interface.py │ │ ├── oauth.py │ │ └── token.py │ │ │ ├── behaviours │ │ ├── __init__.py │ │ ├── unauthenticatedUser.py │ │ └── authenticatedUser.py │ ├── __init__.py │ ├── locustfile.py │ └── settings.py ├── README.md ├── poetry.lock ├── poetry.toml ├── pyproject.toml └── struct.txt Let\u0026rsquo;s take a look at the contents of each file. First, the interface\n## LoadTests/authentication/interface.py import HttpUser from abc import abstractmethod class IAuthUser(HttpUser): def on_start(self): \u0026#39;\u0026#39;\u0026#39;Add the call to authenticate when the user starts\u0026#39;\u0026#39;\u0026#39; self._authenticate_user() @abstractmethod def _authenticate_user(self): \u0026#39;\u0026#39;\u0026#39;Authenticate the user via the auth mechanism\u0026#39;\u0026#39;\u0026#39; Now, let\u0026rsquo;s look at an example with token authentication\nfrom LoadTests.authentication,interface import IAuthUser @dataclass class UserAuthConfig: username: str password: str class TokenAuthUser(IAuthUser): def __init__(self, user_auth_config: UserAuthConfig, *args, **kwargs): self._user_auth_config = user_auth_config self._token = None super().__init__(*args, **kwargs) def _authenticate_user(self): \u0026#39;\u0026#39;\u0026#39;Implement the authentication method using basic auth\u0026#39;\u0026#39;\u0026#39; response = requests.get( \u0026#39;/login\u0026#39;, auth=HTTPBasicAuth(self._user_auth_config.username, self._user_auth_config.password) ) # Grab the token (yes...no validation for this example) token = response.json().get(\u0026#34;token\u0026#34;) Now that we have these classes we can simplify the original authenticatedUser class:\n## LoadTests/behaviours/authenticatedUser.py from LoadTests.authentication.token import TokenAuthUser, UserAuthConfig class AuthenticatedUser(TokenAuthUser): \u0026#39;\u0026#39;\u0026#39;Define a collection of tasks that can be performed by an authenticated user\u0026#39;\u0026#39;\u0026#39; @task def visit_profile(self): \u0026#39;\u0026#39;\u0026#39;Visit the authenticated user\u0026#39;s profile\u0026#39;\u0026#39;\u0026#39; self.client.get(\u0026#39;/profile\u0026#39;) This reduces the scope of the AuthenticatedUser class and makes it easier to set up another class extending the TokenAuthUser. One could imagine having a class for AdminUser and EditorUser and defining behaviours for each type where these both extend TokenAuthUser.\nThis final step to run this new configuration involves importing the behaviour classes into the Locustfile and configuring them:\n## LoadTests/locustfile.py from LoadTests.behaviours import unauthenticatedUser, authenticatedUser from LoadTests.authentication.token import UserAuthConfig from settings import user_auth_config class UnauthenticatedUser(unauthenticatedUser.UnauthenticatedUser): def __init__(self, environment): super().__init__(environment) class AuthenticatedUser(authenticatedUser.AuthenticatedUser): def __init__(self, environment): \u0026#39;\u0026#39;\u0026#39; Get the user_auth_config object from settings.py and pass via super\u0026#39;\u0026#39;\u0026#39; super().__init__(user_auth_config, environment) Note: Locust will only pick up and run classes defined in the locustfile (i.e. not instances of classes so create a subclass of each that is configured). You will notice that both of these classes only take self and environment as constructor args. The AuthenticatedUser class, however, passes a user_auth_config object to its parent when initialized.\nRunning the Final Result You can now run the final result from within the LoadTests/ dir by running:\ncd /LoadTests ## Using the class picker flag will allow you to control via the UI what behaviours/users you want to run in your load test locust --class-picker Your server should come up and you can begin to configure tests! ","permalink":"http://localhost:1313/posts/03-locust-load-testing/","summary":"Big Idea I was recently tasked with configuring automated load tests to validate the health of a service under load and to identify bottlenecks and limits at which the service became overloaded. Up until this point, I had not worked first-hand with any load-testing frameworks. Although there are many great load testing tools out there like JMeter, K6s, and Locust, I decided to get started with Locust as it is a framework I had heard of before and is a pure Python framework (Python is the language I think in right now).","title":"Load Testing Applications with Locust"},{"content":"Big Idea In part 1 of this post, I covered the basics of getting started building my Kubernetes cluster on Raspberry Pis. In particular, I laid out my goals and requirements, the build list, the network topology and setup, and the installation of K3s on each of the nodes. I recommend going back and checking out that post first if you haven\u0026rsquo;t already (Part 1: Building a Bare-metal Kubernetes Cluster on Raspberry Pis).\nA summary from that post is that I created the subnet 10.100.0.0/24 on my home network for the cluster network. Using DHCP I reserved a small address space of IPs for my nodes and statically assigned the node IPs from within that range. Of my four RPis, three will be part of the cluster. The fourth node will not run as part of the cluster but will instead run a TailScale subnet router and Pi-Hole for DNS resolution on the cluster network.\nIn this post, I will review how I configured the Nginx Ingress controller and Cert-Manager for managing HTTPs traffic to my cluster. I will also cover my persistent storage solution that implements PV\u0026rsquo;s in Kubernetes using an SMB share. Finally, I will briefly show my backup strategy that leverages rclone and Backblaze B2 storage.\nAs usual, all of my configurations for deploying charts can be found on my GitHub: https://github.com/atmask/helm-charts/tree/main\nLoad-Balancing, Ingress, and SSL/TLS Management Now that I had my cluster up and running with a CNI installed (I\u0026rsquo;ll do more posts about Calico CNI in the future) I wanted to get the networking setup to access services on my cluster. To achieve this, I added three different installations to my cluster: MetalLB, Nginx Ingress, and Cert-Manager.\nKubernetes has a resource type called Services. Services function as load balancers by providing a single IP for balancing traffic among a backing set of ephemeral pods running a workload. Kubernetes services resources have a few kinds, namely, ClusterIP, NodePort, and LoadBalancer. There are default implementations for the first two service types in Kubernetes but none for LoadBalancer-type services. Most major cloud providers you use will have their own implementation with their Kubernetes offerings that will configure a Load Balancer with a public IP from their platform to manage the incoming traffic to your service. MetalLB is a solution for those running their own Kubernetes clusters to support services of type LoadBalancer.\nNginx-Ingress is an Nginx-based solution for managing network traffic entering the cluster. To use the nginx ingress controller, you expose it behind a LoadBalancer. All incoming traffic can then be routed based on various routing rules such as the path to other services in the cluster. This has the advantage of having a single point for managing TLS termination and in cloud environments can save you the cost additional LBs would incur if you exposed each service via an LB.\nFinally, Cert-Manager. Cert-Manager is an X.509 certificate management service for Kubernetes. It integrates nicely with Let\u0026rsquo;s Encrypt for automatically generating and rotating SSL/TLS certs on domains that you own. It also (with some configuration) integrates with Nginx Ingress for automatically provisioning and managing certificates for new domains and subdomains.\nMetalLB Concepts IPPools MetalLB is not able to give you IP addresses out of thin air. This means that you will need to tell it which IP addresses it can allocate to LoadBalancer services by defining IPPools for your MetalLB installation to use. This IPPool should not overlap with the IP range that the DHCP is configured to assign IPs from. This is where it may be helpful to share the network topology again: External Announcements and Layer 2 Routing MetalLB can assign an IP to your LoadBalancer Kubernetes service, however, it also needs to make the network outside of your cluster pod traffic aware of these IPs to make them routable. MetalLB has two different modes for achieving this goal: BGP and Layer 2. I will focus on Layer 2 mode as that is what I am running and familiar with.\nMetalLB in Layer 2 mode works by taking an available IP from the IPPool that you previously allocated and assigning that IP to a node in your cluster. From an outside perspective, it looks as if the node has multiple IPs on the network. This is called Layer 2 mode because of how it makes use of ARP (address resolution protocol). ARP is a protocol that takes place in layer 2 of the OSI networking model. In short, ARP works by the source machine sending out a broadcast message for the destination IP of the packet it is trying to route. If a machine has that IP leased then it responds to the original ARP request by returning its MAC address. The MAC address is then used in layer 2 networking to send the packet on to the node in the cluster.\nOnce the packet is routed to the node in the cluster then kube-proxy takes over and routes the packet to one of the services. kubey-proxy is an agent that manages iptables in the cluster to support the routing of packets from the virtual IPs of services to the IPs of pods assigned via the CNI. This may be the subject of another post in the future but for now, I\u0026rsquo;d refer you to this article: Kube-Proxy: What is it and How it Works\nDeploying MetalLB Now, to the fun part! MetalLB can be deployed via Helm charts to your cluster. The Helm chart can be found on Artifact Hub. The first install of the MetalLB chart additionally installs custom CRDs to the cluster. CRDs are \u0026ldquo;Custom Resource Definitions\u0026rdquo; and allow for the creation of custom resources (like pods or deployments) but more relevant to a specific application. In the case of MetalLB, we care about the IPAddressPool and L2Advertisement CRDs. After the initial install of the chart, we will want to deploy an IPAddressPool resource to to tell MetalLB what IP range we have set aside in our subnet for LoadBalancer IPs. We will also deploy an L2Advertisement resource that tells Metallb to advertise IPs in that pool via Layer 2 networking.\nI prefer to use Helm\u0026rsquo;s subcharting functionality to keep all of my chart configurations and versions in VCS over time but, in general, the installation process would look like this:\nhelm install -n metallb metallb oci://registry-1.docker.io/bitnamicharts/metallb --create-namespace Then create the IPAddressPool in ippool.yaml\n# The address-pools lists the IP addresses that MetalLB is # allowed to allocate. You can have as many # address pools as you want. apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: # A name for the address pool. Services can request allocation # from a specific address pool using this name. name: lb-ip-pool namespace: metallb-system spec: # A list of IP address ranges over which MetalLB has # authority. You can list multiple ranges in a single pool, they # will all share the same settings. Each range can be either a # CIDR prefix, or an explicit start-end range of IPs. addresses: - 10.100.0.50-10.100.0.75 and the L2Advertisement in advertisement-l2.yaml\napiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: ip-pool-advertisement namespace: metallb-system spec: ipAddressPools: - lb-ip-pool then apply both files to the cluster:\nkubectl apply -n metallb -f ippool.yaml kubectl apply -n metallb -f advertisement-l2.yaml Note: This is the quick and dirty way to do this. I reccommend checking out my repo linked at the start of this article to see how subchartting can be used for maintaining the configuration of third-party charts.\nValidate with LoadBalancer If you don\u0026rsquo;t have any applications deployed and want to validate that the metallb installation is working you can apply the following LoadBalancer Service to your cluster and verify that an IP from your IPPool is attached to the service as an external-ip\n### lb-svc.yaml apiVersion: v1 kind: Service metadata: name: my-service namespace: default spec: selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 8080 type: LoadBalancer and apply with kubectl apply -f lb-svc.yaml. Then use kubectl get svc to verify that an external-ip has been assigned to the service named \u0026ldquo;my-service\u0026rdquo;.\nDeploying Nginx-Ingress Now that we can provision IPs for an implementation of LoadBalancer-type services we can move on to our installation of Nginx Ingress. Yes, technically, NodePort could have been used instead of setting up MetalLB but this is about learning new things! The Nginx Ingress controller will act as a single point of entry for all traffic to workloads running in my cluster. Nginx Ingress supports host and path-based routing and I will make use of this when setting DNS records for my apps later on. A large benefit of using Nginx Ingress as a single point of entry for all incoming traffic is that I can integrate Nginx Ingress with Cert-Manager so that it is also the single point for managing all TLS termination of incoming traffic. This reduces the management overhead for me.\nAs with MetalLB, my configuration of the Nginx Ingress controller chart can be found in my charts repo linked at the top of this post. There is a lot less configuration to do for this deployment. I will provide a quick and dirty installation here as well though. All that you need to configure via the values file is the service resource that will act as the load balancer to the ingress controller.\n### nginx-ingress-values.yaml controller: service: enabled: true type: \u0026#34;LoadBalancer\u0026#34; annotations: metallb.universe.tf/address-pool: lb-ip-pool # Add the annotation so that metallb will use the previously configured ippool and then install:\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm install -n nginx-ingress nginx-ingress ingress-nginx/ingress-nginx -f nginx-ingress-values.yaml --create-namespace Hura-ah! The Nginx Ingress controller should now be installed. It will watch of the creation ingress resources in the cluster. ingress resources are typically deployed with your application and configure how the Nginx Ingress Controller should route traffic to that application.\nConfiguring Cert-Manager As of right now, with MetalLB and the Nginx Ingress controller set up, I would be ready to access my applications. The problem is, any web browser will give me a page saying that the page I am trying to access is insecure and I would have to click through to access my applications. This is an annoyance and so I have set up Cert-Manager to solve this problem.\nCert-Manager is a tool that can be integrated with Nginx Ingress to automatically provision and rotate SSL/TLS certs for your domains. The SSL/TLS certs are used to encrypt the HTTP traffic between your servers and clients. This is what gives us HTTPs.\nAn important thing to know before setting up Cert-Manager is that SSL/TLS certificates can only be issued for domains that you own. When Cert-Manager attempts to provision or rotate a cert for your service it must pass one of two tests known as ACME challenges (DNS-01 or HTTP-01) to verify that you are the owner of the domain for which you are provisioning a certificate.\nInstalling Cert-Manager Cert-Manager, like MetalLB, has a set of CRDs that are needed for configuring the installation. This means that I had to do the initial Helm install to get the CRDs and then a subsequent Helm upgrade to add configurations. There is one additional CRD needed when setting up Cert-Manager. The required CRD, for my use case and setup at least, is the ClusterIssuer. The ClusterIssuer configures which Certificate Authority (CA) Cert-Manager will use to generate the SSL/TLS certificates. In my installation, I have used Let\u0026rsquo;s Encrypt as my CA. Let\u0026rsquo;s Encrypt provides free certificates and has a generous quota on their production service.\nThe initial Cert-Manager installation can be added as follows:\nhelm repo add jetstack https://charts.jetstack.io --force-update helm install cert-manager -n cert-manager --version v1.15.1 jetstack/cert-manager --set installCRDs=true --create-namespace After the initial installation, I deployed the ClusterIssuer. As I mentioned earlier, certificates can only be issued for domains that you own. This means that the CA you choose will attempt to execute a DNS-01 or HTTP-01 challenge successfully before provisioning your cert. DNS-01 is more secure so I cover that here. With DNS-01 you must configure your ClusterIssuer with an API key for the registrar where you manage your domain. In my case, this is Cloudflare. The DNS-01 challenge works by the CA requesting that you configure a TXT record on your domain with specific values that the CA provides under the subdomain _acme-challenge.\u0026lt;YOUR_DOMAIN\u0026gt;. If the CA validates that this has been done, then your ownership of the domain is verified and the certificate is issued. Using my provided API key, Cert-Manager will do this all on my behalf.\n## cluster-issuer.yaml apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt spec: acme: # You must replace this email address with your own. # Let\u0026#39;s Encrypt will use this to contact you about expiring # certificates, and issues related to your account. email: \u0026lt;your email\u0026gt; server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: # Secret resource that will be used to store the account\u0026#39;s private key. name: \u0026lt;arbitrary secret name where your acme account key will be stored once generated\u0026gt; # Add a single challenge solver solvers: - dns01: cloudflare: apiTokenSecretRef: name: registrar-api-key key: api-token ## secret.yaml apiVersion: v1 kind: Secret metadata: name: registrar-api-key type: Opaque stringData: api-token: {{ .Values.cloudflareApiToken }} Note: If you are setting this up for the first time it is recommended to use the Let\u0026rsquo;s Encrypt staging servers to not waste your quota on the production servers. The staging server can be gotten by replacing acme-v02 with acme-staging-v02 in the server configuration.\nWith Cert-Manager and a ClusterIssuer installed, I was now able to create new ingress resources containing routing rules and integrated with Cert-Manager via annotations for the automatic provisioning of SSL/TLS certs. Using the API token for Cloudflare, Cert-Manager completes the DNS-01 ACME challenge on my behalf.\nLocal DNS Management with Pi-Hole One more small detail about my cluster pertains to DNS records. I did not want all the DNS records for my services available on the public internet. Although the services running on my local network would be unreachable to anyone on the public internet, storing the DNS records with the private IPs for my hosted services in Cloudflare or any other public DNS provider would have meant that anyone could discover the private IPs at which I host services. For me, this was not ideal. This was the motivation for setting up Pi-Hole as a DNS server on my home network. Pi-Hole will act like an Azure Private DNS Zone or Private Hosted Zones in AWS Route53.\nThe first step to achieving this goal was installing Pi-Hole on one of my Pis. I chose to do the installation on stoneward (refer to the network diagram for the naming of my nodes). This is the same Pi that runs TailScale and does not serve any role in the K3s cluster. After completing the Pi-Hole installation I was able to navigate to Local DNS \u0026gt; DNS Records in the Pi-Hole admin web UI and configure DNS records for reaching the nginx-ingress controller and other common IPs such as the cluster node IPs.\nNote: I had to adjust the DNS server\u0026rsquo;s Interface Settings in the Pi-Hole admin web UI. By default, only local requests from devices one hop away are allowed which does not work with my Tailnat. Since my DNS server is only accessible on my private home network I changed this setting to permit all origins for DNS queries.\nThe second and final step for setting up the private DNS requires configuring TailScale. The TailScale admin portal allows you to configure the DNS nameserver to use for anyone connecting to your Tailnet. This must be updated with the IP of stoneward and the override local DNS setting set to true. When this is configured. All DNS requests for users connected to your Tailnet will be routed via the Subnet Router\u0026rsquo;s advertised CIDR range to the DNS running at home. This will allow only users connected to my local network to resolve the DNS records for services running in my homelab.\nPersistent Storage The final part of my cluster configuration that I will cover in this post is my implementation of persistent storage. I wanted to be able to have a reliable persistent storage solution as I intend to store data that matters to me on my services. However, I also wanted a solution that required a small amount of effort and minimal cost. This ruled out using Kubernetes hostPath since the micro SD cards that act as my primary storage on the RPis in not very reliable. The hostPath solution also has the issue that data would be tied to a specific node meaning that pods could not be scheduled interchangeably on any node. Exploring a distributed file storage solution such as Rook Ceph or Longhorn was interesting to me but not something I really had the bandwidth to explore and seemed overkill for my use case.\nIn order to de-couple my storage solution from the nodes I chose to set up a 1TB SSD as an SMB share that could be mounted by any node in the cluster via the SBM CSI driver. The SMB CSI driver is a Kubernetes CSI implementation that enables pods to access an SMB share via Persistent Volumes and Persistent Volume Claims.\nCreating the SMB share When I set up my cluster, I only had one node that had a significant amount of RAM. This was my k3s master node, bondsmith, which had 8Gb of memory. As a result, I decided to expose the 1TB SSD from this node\nI connected my 1 TB SSD to the Pi through a SATA to USB converter cable. The cluster case I used also had a convenient place to mount the drive on the same ejectable rack that the Pi was connected to. After powering on the PI with the PoE connection I connected the Tb drive and configured the SMB share.\nThe first part of setting up the SMB share was installing Samba to the bondsmith node. After installation I mounted the TB drive to the /nas dir at the root of my machine. Using the lsblk command I was able to find my drive on the system and then use the mount command to mount the drive to the /nas dir.\n## Mount the drive sudo mount /dev/sda2 /nas Note: It\u0026rsquo;s a good idea to create a new user and group for access to the SMB share on the system. I created a user and group for SMB users and changed the access permissions and ownership of the mounted drive to that user\nOnce the drive is mounted you can configure Samba to share the drive to the network. This can be done by configuring and smb.conf file and restarting the smb service.\n## /etc/samba/smb.conf [nas] path=/nas writeable=Yes create mask=0777 directory mask=0777 public=no The last step is adding the newly created smb user to Samba using smbpasswd.\nSetting up the SMB CSI Driver The SMB CSI driver was easily installed into the cluster via the Helm chart. In order to enable the use of PVCs/PVs it was necessary to configure a StorageClass resource along with the Helm chart. The storage class specifies the connection details for mounting the SMB share. The storage class is then used in PVCs to automatically connect to the network drive and access data.\n## storageclass.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: smb provisioner: smb.csi.k8s.io parameters: source: //\u0026lt;ip of the node\u0026gt;/\u0026lt;path to the smb share\u0026gt; # if csi.storage.k8s.io/provisioner-secret is provided, will create a sub directory # with PV name under source csi.storage.k8s.io/provisioner-secret-name: smbcreds csi.storage.k8s.io/provisioner-secret-namespace: default csi.storage.k8s.io/node-stage-secret-name: smbcreds csi.storage.k8s.io/node-stage-secret-namespace: default reclaimPolicy: Delete # available values: Delete, Retain volumeBindingMode: Immediate mountOptions: - dir_mode=0777 - file_mode=0777 - uid=1001 - gid=1001 - nobrl # details on using nobrl: ## https://learn.microsoft.com/en-us/troubleshoot/azure/azure-kubernetes/storage/mountoptions-settings-azure-files#other-useful-settings ## https://github.com/dani-garcia/vaultwarden/issues/846 You will also need to create a secret containing the username and password for connecting to the SMB drive:\nkubectl create secret generic smbcreds --from-literal username=\u0026lt;USERNAME\u0026gt; --from-literal password=\u0026lt;PASSWORD\u0026gt; Testing PVC/PV Creation After the above is installed and set up, I was able to validate that storage was provisioned when specifying the persistent volume claim.\napiVersion: apps/v1 kind: StatefulSet metadata: name: ss-smb labels: app: busybox spec: replicas: 1 template: metadata: labels: app: busybox spec: containers: - name: ss-smb image: alpine:latest command: - sleep - infinity volumeMounts: - name: smb mountPath: \u0026#34;/sc/smb\u0026#34; tolerations: - key: \u0026#34;node.kubernetes.io/os\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; updateStrategy: type: RollingUpdate selector: matchLabels: app: busybox volumeClaimTemplates: - metadata: name: smb annotations: volume.beta.kubernetes.io/storage-class: smb spec: accessModes: [\u0026#34;ReadWriteMany\u0026#34;] resources: requests: storage: 10Gi Backups with rclone and Backblaze I wanted to make sure that I keep backups of all my data somewhere cheap in case I need to do some disaster recovery. I chose to use Backblaze since it has an S3 compatible API and is cheap. There are many guides for authenticating to Backblaze with rclone and k3s so I will just add the commands here in case I need to reference them in the future.\nNote: I did not use an in-cluster back-up tool like Velero and Rustic because they require snapshot capabilities from the CSI driver and the SMB CSI driver does not support this.\nBacking up the SMB drive rclone sync -P /nas/ b2:\u0026lt;bucket name\u0026gt; Backing up etcd k3s etcd-snapshot save --s3 --s3-endpoint \u0026lt;backblaze endpoint\u0026gt; --s3-bucket \u0026lt;backblaze bucket\u0026gt; --s3-access-key \u0026lt;bb access key\u0026gt; --s3-secret-key \u0026lt;bb secret key\u0026gt; ","permalink":"http://localhost:1313/posts/02-bare-metal-k3s-on-rpi-part-2/","summary":"Big Idea In part 1 of this post, I covered the basics of getting started building my Kubernetes cluster on Raspberry Pis. In particular, I laid out my goals and requirements, the build list, the network topology and setup, and the installation of K3s on each of the nodes. I recommend going back and checking out that post first if you haven\u0026rsquo;t already (Part 1: Building a Bare-metal Kubernetes Cluster on Raspberry Pis).","title":"Part 2: Building a Bare-metal Kubernetes Cluster on Raspberry Pis"},{"content":"Big Idea From the start of my career, I have been fascinated by Kubernetes. I love distributed systems and the rich history of how we have arrived where we are today with distributed computing. We live in the ongoing evolution of our communal vision for cloud computing. In the early years, the vision was to present a homogenous Unix-like interface for managing an underlying collection of servers such as BOINC. Now, we live in the world of many small virtualized unix environments distributed across servers and sharing compute.\nOne of the main drivers of our current state has been the advent of containerization and container orchestration. The goal of this blog is to go over the considerations and design of my bare-metal Raspberry Pi Kubernetes cluster. This project was my adventure in going beyond being a user of Kubernetes offerings from various cloud providers to understanding the physical magic behind the scenes.\nFull disclaimer: This project is largely based-off Anthony Simon\u0026rsquo;s project for a similar build. I found that his blog post lacked a lot of details though so I want to capture those missing parts here and go into more detail about my setup. You can find his great post here!\nYou can find the Ansible scripts and Helm charts that I manage for this project on my Github:\nHelm Charts Ansible Scripts Goals \u0026amp; Requirements Before diving into the build list, architecture, and design for my build I want to review what, for me, were the goals and requirements for setting up this project.\nPortability First, portability. I am in a season of life that is nomadic. I am in different apartments for a year or two at a time. I want a build that I can easily unplug, bring somewhere else, and plug in without needing any extra steps for setup.\nIsolation \u0026amp; Security Second, and closely related, is isolation. I want the network that my cluster runs on to be on a subnet isolated from the LAN network to which it connects. I want all IPs to be in their own non-overlapping address space. I also don\u0026rsquo;t want my services available publicly or to anyone connected to the LAN of my home network. They should only be accessible via a VPN connection to the cluster network or wireless/wired connection to the cluster LAN.\nPersistent Storage \u0026amp; Back-ups Third, I wanted my cluster to support some implementation PVs and PVCs for data persistence. I wanted this to be affordable and to be reliable. This ruled out buying SSD storage for each node and using a distributed file store like Rook/Ceph or Longhorn. It also ruled out using hostPath storage on SD cards. (Spoiler) My final result uses a single Terabyte SSD that is running as an SMB share which can be mounted via the SMB csi.\nHTTPs My fourth requirement is that all of my services should be available over an HTTPs connection. Sure, the VPN is encrypted, however, I want TLS termination at the cluster and not only the VPN. Further, I don\u0026rsquo;t want browsers complaining that the site I am visiting is not secure. That is a bother for me and a red flag for any friends or family who connect to my services.\nDNS Lastly, I want my services accessible via DNS records when a user is connected via VPN. I want the DNS server to sit on the cluster LAN network and become the primary DNS server for users when they connect to the network. This keeps my A records off of public DNS servers.\nArchitecture The following diagram lays out my planned network topology for this build. I am a big fan of Brandon Sanderson\u0026rsquo;s Stormlight Archives and so I have named my nodes after the Radiant orders.\nI aim to have a 10.100.0.0/24 CIDR allocated to my cluster network. I will have a dedicated router that manages this subnet. The router and nodes will all share a wired ethernet connection through an L2 Gigabit PoE network switch. Within that cluster network, I will configure the DHCP server to assign IPs to the network devices from a subset of the network IPs available. This will allow me, later on, to use another non-overlapping IP range within the cluster CIDR for MetalLB. Kubernetes does not provide a default implementation for allocating IPs to LoadBalancer services. MetalLB is one solution to this problem that I will explore in more depth later on. From the perspective of my cluster network, the home network will be the WAN. All internet-bound traffic will traverse the cluster router gateway and then my home router.\nAnother detail of this design is the SMB share. I have a 1TB SSD physically mounted to one of my RPi nodes. This RPi exposes the 1TB drives as an SMB share that can be mounted by other devices on the network. There is a Kubernetes SMB Container Storage Interface (CSI) driver that supports mounting SMB shares via PVCs. This is how I will implement my poor man\u0026rsquo;s persistent storage.\nNote: This is not intended to be a HA cluster. I only have a single master node. The numbers aren\u0026rsquo;t ideal for consensus. In this build, I just want to learn the basics.\nBuild List The following is my build list for the project:\nUCTronics RPi Cluster Case x1 Raspberry Pi 4b 8Gb x1 Rapsberry Pi 4b 2Gb x3 1 TB SSD x1 SATA to USB 3.0 Adapter x 1 TP-Link 5 port Gigabit PoE Network Switch x1 TP-Link Nano Router x1 0.3M Ethernet Cables x4 RPi PoE Hat x4 Note: I do not receive any commission when you purchase via the above the links. These are just what worked for my build and are what I recommend.\nNetworking With a project like this, you need to start small and work up. Realistically, this means breaking up your end goal into small problems that you can manageably troubleshoot and solve as you go. Trying to take on too much with so many variables and unknowns in one swing will be fatal for a project of this kind. I have broken down this section into the incremental steps I took to accomplish my vision for the networking. These steps were goals I wanted to achieve before approaching the problem of setting up Kubernetes.\nTo start, I linked my TP-Link router and each of the Pis to the network switch via ethernet cables. Using the PoE hat on each Pi the PoE network switch was also able to power the Pis without the need for additional cables.\nConfiguring the Cluster Router When setting up the TP-Link Router the goal is to create a subnet. The TP-Link router will be a gateway to my home network LAN and from there traffic will be routed to the internet via my home network route. To do this, I configured the TP-Link Router in WISP mode. In WISP mode, the router adopts the home network as the WAN network and then broadcasts its own LAN network to which wired/wireless devices can connect. This results in two isolated networks.\nWAN Settings In this configuration, your TP-Link cluster router will be assigned an IP on your home network. The gateway for the cluster router will be the IP of your home network router.\nLAN Settings In the TP-Link router\u0026rsquo;s LAN settings, you\u0026rsquo;ll need to configure the LAN. This is where you can specify the subnet that your cluster nodes will run on. I chose to use the 10.100.0.0/24 CIDR for my cluster network. This gives me 254 IPv4 addresses to work with with (256 minus the broadcast ip and network address) which is more than enough for my little cluster.\nDHCP Settings In the TP-Link router DHCP settings you\u0026rsquo;ll want to configure the IP range (within your LAN subnet) that the DHCP server can pull from when assigning IPs to devices joining the cluster network. A DHCP server is a Dynamic Host Configuration Protocol server. When new devices join a network they send out a discovery call to the DHCP server. The DHCP server then returns an offer containing an IP for the devices to use in the network and other configurations such as the DNS server to use.\nLater on, we\u0026rsquo;ll come back here and configure the DNS.\nStatic Node IPs For this build, I did not want to bother with IPs changing for the nodes in my cluster. For this reason, I assigned each node linked to the network a static IP. You can do this in the DHCP configuration settings of the router so that when the nodes get a new DHCP lease they always get the same IP\nAdding a TailScale Subnet Router Out of my four Raspberry Pis, I have committed three to the cluster and one to running a TailScale subnet router and Pi-Hole. The stoneward node is the Pi that I have chosen to use for hosting my TailScale subnet router and Pi-Hole DNS server. TailScale is a VPN service that builds on top of simplifies Wireguard by delegating the management of peer certificates among new peers to the TailScale control plane. Using TailScale you can run a node as a Subnet Router to route traffic from other users of the VPN network to IP space behind the subnet router. I will take advantage of this functionality by converting the stoneward node into a subnet router that handles routes to my cluster network\u0026rsquo;s CIDR range. This means, that when connected to the VPN, I can reach all of my nodes and services without exposing them to the public internet.\nThe install scripts for TailScale can be found in my Ansible repository. After installing TailScale and advertising my cluster subnet range (note: you have to also approve this node advertising that range in the TailScale Admin console) I then validated that my personal dev laptop could ssh into each of the other nodes linked to my subnet via the PoE network switch.\nK3s For this project I chose to run K3s. K3s is a lightweight Kubernetes binary that runs with significantly smaller memory requirements. I wanted to find a lightweight solution that didn\u0026rsquo;t feel like a compromise and so I was satisfied to run k3s as it is fully Kubernetes compliant, included some functionality out of the box like coredns, and could use an etcd data store.\nInstallation For my K3s installation, I chose to override some of the default tools in favour of tools with which I had more experience. In particular, I replaced the default Flannel CNI with Calico, the Traefik Ingress controller with Nginx Ingress, and ServiceLB with MetalLB. To see all of the installation scripts check out my ansible automation linked at the start of this article. The configurations for my custom tools were installed via Helm and all of the configurations can be found in the Helm Chart repo also linked alongside my Ansible repo.\nOne thing to note about my installation of K3s. K3s supports two types of nodes: k3s-servers and k3s-agents. The k3s-server nodes are responsible for the control plane and datastore components. The k3s-agents do not have any of those responsibilities and just run the kubelet, CRI, and CNI. I chose my 8Gb Raspberry Pi as the single k3s-server node for this cluster. The reasoning for this was two-fold, First I wanted to use etcd since I haven\u0026rsquo;t before. Second, I only had a single SSD for this project and did not want any datastore components running on nodes that only had an unreliable SD card.\nConclusion At this point in time, I had the cluster networking set up and K3s installed on each node. Two of the Raspberry Pis, lightweaver and windrunner, were configured as k3s-agent nodes and Bondsmith, my 8Gb Pi, was running as the single k3s-server. With that, I will bring this post to a close. In part 2, I will review my configuration of Nginx Ingress, MetalLB and, and Cert-Manager for managing incoming traffic to my cluster services. Part 2 will also cover how I configured my 1 TB SSD drive as an SMB share to dynamically provision persistent volumes for my workloads.\n","permalink":"http://localhost:1313/posts/02-bare-metal-k3s-on-rpi-part-1/","summary":"Big Idea From the start of my career, I have been fascinated by Kubernetes. I love distributed systems and the rich history of how we have arrived where we are today with distributed computing. We live in the ongoing evolution of our communal vision for cloud computing. In the early years, the vision was to present a homogenous Unix-like interface for managing an underlying collection of servers such as BOINC. Now, we live in the world of many small virtualized unix environments distributed across servers and sharing compute.","title":"Part 1: Building a Bare-metal Kubernetes Cluster on Raspberry Pis"},{"content":"Big Idea The goal of this post is to capture the steps required to get started with Hugo and GitHub Pages. Hugo is a Go-based static site generation tool. GitHub Pages is a feature of GitHub that allows anyone with a GitHub account to host a static site.\nPart 1: Setting up GitHub Pages In order to serve your site, you will need somewhere to host it. GitHub offers a free service called GitHub Pages that we will use for this purpose. GitHub Pages offers the free hosting of static website content. This means we will be able to build our Hugo site into a static site and then serve that via GitHub Pages.\nCreate GitHub Pages Repo To get started with GitHub Pages for your blog, you will first need a repo in which you\u0026rsquo;ll store your website content. GitHub Pages offers websites for individual projects or for your user. For this tutorial, we will use the user GitHub Pages. The first step to creating this Pages site is creating a repo in your GitHub account that follows the naming scheme: \u0026lt;username\u0026gt;.github.io (substitute \u0026lt;username\u0026gt; with your GitHub username). Select Initialize this repository with a README and then create the repo.\nNote: Your repository must be a public repo for Pages to work.\nAfter creating your repository, navigate to the repository\u0026rsquo;s main page and click on Settings: From the Settings page navigate to the Pages under Code and Automation on the side menu: From here you will want to change your Build and deployment configuration to GitHub Actions. This will be required later when we want to specify a GitHub Actions workflow to build our static site content with Hugo.\nFor now, leave the Custom domain configuration alone. We will return to this at a later step.\nPart 2: Setting Up Hugo The following section covers getting your static site running on your local machine. This will allow you to modify your themes and posts from your editor of choice and see updates via the local Hugo dev server.\nInstall Hugo to your local machine To get started with Hugo on your machine you\u0026rsquo;ll need to first install Hugo. On macOS, you can do this via Brew:\nbrew install hugo Create a New Hugo Site Locally To start a new Hugo project run:\nhugo new site \u0026lt;github-username\u0026gt;.github.io --format yaml Note: Using --format yaml is optional. The default config format for Hugo is toml.\nThis will create a new directory named \u0026lt;github-username\u0026gt;.github.io that is pre-populated with the Hugo starter boilerplate. You don\u0026rsquo;t have to use the site name \u0026lt;github-username\u0026gt;.github.io. You can change it to whatever you would like. However, in the case of GitHub Pages, you created your Pages site in a repo named \u0026lt;github-username\u0026gt;.github.io. To keep naming consistent use the name of the existing repo.\nConnect your local Hugo Project to the Git Repo Now that you have created a new Hugo site you will want to connect it to the GitHub repository you created earlier. To do this we will first initialize your new local project as a git project. This can be done by entering your project directory and then running git init\ncd \u0026lt;github-username\u0026gt;.github.io/ git init This enables version control for your project. Let\u0026rsquo;s create an initial commit of the Hugo site and push to GitHub:\ngit add . git commit -m \u0026#34;Initial commit\u0026#34; git branch -M main git remote add origin \u0026lt;git clone url\u0026gt; git push -u origin main --force ## Use force in this case to override the README in github with new history Note: You can get the git clone url by navigating to your repository, selecting Clone and grabbing the https or ssh clone link. Select the ssh link if you have ssh set up for GitHub. This will keep you from having to enter your credentials on each push.\nAdd a Hugo Theme to your site By default, Hugo does not include a theme for your site. This is where you get to pick how you want your site to look once built and deployed. You can find a complete list of themes on the Hugo themes page. For this example, I will use the theme PaperMod as that is the theme of this blog.\nOnce you have selected a theme, you will want to clone that theme into the /themes directory contained within the Hugo project created above. There are two common ways that others online will recommend doing this. One way requires downloading the theme\u0026rsquo;s repo as a zip file from GitHub, extracting the contents and moving them into the /themes directory. This method does not maintain the git history of the selected theme. It means that as the upstream theme repo changes you will not be able to pull those changes via git. The second method involves cloning the theme repo into /themes and declaring it as a git submodule within the enveloping git repo you created earlier. I don\u0026rsquo;t intend on maintaining my theme heavily so I will not bother with the latter approach. I also find downloading and unzipping tedious. I recommend cloning your theme into /themes and then dropping the .git management from the clone. This is done as follows:\n## Clone PaperMod theme to /themes/PaperMod. Only grab depth 1 git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod ## Remove the git history from the cloned PaperMod repo rm -rf /themes/PaperMod/.git Now that you have a theme, you need to tell Hugo to use it. To do so, edit your hugo.yaml and add\ntheme: - PaperMod ## If you chose a different theme put the name of the theme here. This is the folder under /themes that contains the theme Run Hugo Locally You can now run your site locally by running hugo server from the terminal within your project. This brings up your site at http://localhost:1313/.\nYou can change the title on your site by editing the hugo.yaml file. All PaperMod features and customizations can be found documented here: PaperMod Features\nDeploy to GitHub Pages Next, we will deploy to GitHub Pages. Remember that earlier we changed the Build and deployment setting to GitHub Actions. This means that we need to specify a GitHub Actions workflow for GitHub Runners to execute when you push your repo. GitHub Actions are a series of jobs that will be performed on your code base when you push to GitHub. In our case we will use GitHub Actions to build our Hugo site and deploy to Pages. To do this we must create the following workflow file in our repo: \u0026lt;github-username\u0026gt;.github.io/.github/workflows/hugo.yaml\n# Sample workflow for building and deploying a Hugo site to GitHub Pages name: Deploy Hugo site to Pages on: # Runs on pushes targeting the default branch push: branches: - main # Allows you to run this workflow manually from the Actions tab workflow_dispatch: # Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages permissions: contents: read Pages: write id-token: write # Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued. # However, do NOT cancel in-progress runs as we want to allow these production deployments to complete. concurrency: group: \u0026#34;Pages\u0026#34; cancel-in-progress: false # Default to bash defaults: run: shell: bash jobs: # Build job build: runs-on: ubuntu-latest env: HUGO_VERSION: 0.128.0 steps: - name: Install Hugo CLI run: | wget -O ${{ runner.temp }}/hugo.deb https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_linux-amd64.deb \\ \u0026amp;\u0026amp; sudo dpkg -i ${{ runner.temp }}/hugo.deb - name: Install Dart Sass run: sudo snap install dart-sass - name: Checkout uses: actions/checkout@v4 with: submodules: recursive fetch-depth: 0 - name: Setup Pages id: Pages uses: actions/configure-Pages@v5 - name: Install Node.js dependencies run: \u0026#34;[[ -f package-lock.json || -f npm-shrinkwrap.json ]] \u0026amp;\u0026amp; npm ci || true\u0026#34; - name: Build with Hugo env: HUGO_CACHEDIR: ${{ runner.temp }}/hugo_cache HUGO_ENVIRONMENT: production TZ: America/Los_Angeles run: | hugo \\ --gc \\ --minify \\ --baseURL \u0026#34;${{ steps.Pages.outputs.base_url }}/\u0026#34; - name: Upload artifact uses: actions/upload-Pages-artifact@v3 with: path: ./public # Deployment job deploy: environment: name: github-Pages url: ${{ steps.deployment.outputs.page_url }} runs-on: ubuntu-latest needs: build steps: - name: Deploy to GitHub Pages id: deployment uses: actions/deploy-Pages@v4 After adding this run:\ngit add . git commit -m \u0026#34;Add theme and release workflow\u0026#34; git push This will push your changes to GitHub. From the GitHub repo page for your project you can open the Actions tab to see your defined workflow running. Once this has completed you can navigate to \u0026lt;github-username\u0026gt;.github.io to view your site! This could take ~10 mins to become visible so don\u0026rsquo;t worry if you don\u0026rsquo;t see it right away.\nPart 3: Adding a Custom Domain To add a custom domain you will need to follow a few steps. First, if you do not have one already, you will need to purchase a domain from a registrar such as Cloudflare, GoDaddy, or NameCheap. Doing this is beyond the scope of this blog and there are multitudes of online guides that will explain this in more detail than I can here.\nSpecify Domain in GitHub Pages After acquiring a domain, return to the Pages tab under Settings \u0026gt; Build and Automation \u0026gt; Pages in GitHub. Here you will see the Custom domain option. Add your domain to the custom domain settings. This can be the Apex domain such as fallow.app if you want your blog to be the root page of your domain. If you\u0026rsquo;d prefer to host your blog on a subdomain such as blog.fallow.app then enter that as your custom domain.. Replace fallow.app with your domain.\nConfigure CNAME Record in your DNS Provider. The registrar from whom you purchased your domain will have DNS settings available for your domain. There are two main DNS record types A records and CNAME records. An A record points to an IP. CNAME records are aliases to other domains. In our case we will create a CNAME record to our GitHub Pages domain. If you are doing this for the subdomain blog.\u0026lt;your domain\u0026gt;, then add blog as your CNAME record and \u0026lt;github-username\u0026gt;.github.io as your target. If you are doing this for the Apex domain (i.e. no subdomain), then use @ (this represents the apex domain) instead of blog.\nNote: DNS records take a while to propagate. You will no longer be able to reach your blog at \u0026lt;github-username\u0026gt;.github.io and it may take 24hrs for your site to become available. In my experience, it has never been that long and has taken at most 30 mins.\n","permalink":"http://localhost:1313/posts/01-getting-started-w-hugo/","summary":"Big Idea The goal of this post is to capture the steps required to get started with Hugo and GitHub Pages. Hugo is a Go-based static site generation tool. GitHub Pages is a feature of GitHub that allows anyone with a GitHub account to host a static site.\nPart 1: Setting up GitHub Pages In order to serve your site, you will need somewhere to host it. GitHub offers a free service called GitHub Pages that we will use for this purpose.","title":"Getting Started with Hugo \u0026 GitHub Pages"}]